#实验：
##描述
Buy300_2CC_Scaler50_PolicyLossnew_shift001_RT_AV1_E_2D_try42
Buy300_2CC_Scaler50_PolicyLossnew_shift001_RT_AV1_E_2D_try43
都是基于 Buy300_2CC_Scaler50_PolicyLossnew_shift001_RT_AV1_E_2D_try4
主要区别是  LOSS_V 从 0.5 改到 0.05 
##结果观察：
使jointloss 主要受 policy loss 影响



#实验：
##描述
Buy300_2CC_Scaler50_PolicyLossnew_shift001_RT_AV1_E_2D_try43 基于 Buy300_2CC_Scaler50_PolicyLossnew_shift001_RT_AV1_E_2D_try4
"Brain_optimizer": 从 "Adam" 改成 "Adagrad",
##结果观察：
Adagrad 比 Adam 好， SDG 好像无作用


#实验：
##描述
Buy300_2CC_Scaler50_PolicyLossnew_shift001_RT_AV1_E_2D_try43_2 基于  Buy300_2CC_Scaler50_PolicyLossnew_shift001_RT_AV1_E_2D_try43
加了三层卷积和加大了全连接 
##结果观察：
发现模型加卷积层和全连接层加大会使学习失去效果， 具体表现是1. CC eval 结果在来回震荡 

#实验：
##描述
a432_1 和 a432_2 是基于  Buy300_2CC_Scaler50_PolicyLossnew_shift001_RT_AV1_E_2D_try432
a432_1： lv 和 sv 都减少了一层卷积
a432_2:  全连接层 加了一层
##结果观察：
CC 都是收敛到盈亏平衡线， 也就是最终不交易了
policy loss 起了主导作用， 一直持续下降到了负值， 并且加上value loss 后还把 total loss 带到了负值


#实验：
##描述
a432_1_1 和 a432_1_2 是基于 a432_1
a432_1_1 把  改 "train_shift_factor":0.00  保持  "train_scale_factor":50,
a432_1_2 把  改 "train_shift_factor":0.00  "train_scale_factor":20,
##结果观察：
CC 在亏30 万附近震荡
两个的policy loss 没有下降， 但有上升趋势

#实验：
##描述
a432_12_1 和 a432_12_2 是基于 a432_1_1 和 a432_1_2
把    "LOSS_V": 从 0.05 改 到 0.01
把    "LOSS_ENTROPY": 从 0.01 改 到 0.005,

##结果观察：
CC 在亏30 万附近震荡
两个的policy loss 没有下降， 但有上升趋势
这个实验是在 loss_V 改成 0.02 不成功后 再改成 0.01 然后相应 把 LOSS_entropy 改成 0.005

#实验：
##描述
把reward 设置在 0 到 1 之间 不是原来的 -1 到 1 之间
no action, tinpai 和buy 都是 0
shift 后， sell 负profit reward 是 0 正profit reward是 1
借用 原来 config train_flag_punish_no_action True 来激活train 时用新的reward strategy


同时做两个实验， 
一个是基于原来的sell strategy， 用新的reward  直接训练新的 buy strategy
基于 a432_1 的 nr_b
gpu0
顺便把batch 从 150 改回 300

一个是用新的reward 训练新的sell strategy 
基于 Sell300_2_Scaler50_PolicyLossnew_shift001_RT_AV1_2D 的 nr_s
gpu1

##结果观察：

两个的policy loss 没有下降， 但有上升趋势
这个实验是在 loss_V 改成 0.02 不成功后 再改成 0.01 然后相应 把 LOSS_entropy 改成 0.005
CC 在亏30 万附近震荡


#实验：
##描述
把reward 设置在 -10 到 10 之间 不是原来的 -1 到 1 之间
no action, tinpai 和buy 都是 0
shift 后， sell 负profit reward 是 0 正profit reward是 1
借用 原来 config train_flag_punish_no_action True 来激活train 时用新的reward strategy
sacler 100 clip at [-10,10] 这样 0.02 就是 2 0.1 就是10 激励 大的

同时做两个实验， 
一个是基于原来的sell strategy， 用新的reward  直接训练新的 buy strategy
基于 a432_1 的 nr_b
gpu0
顺便把batch 从 150 改回 300

一个是用新的reward 训练新的sell strategy 
基于 Sell300_2_Scaler50_PolicyLossnew_shift001_RT_AV1_2D 的 nr_s
gpu1
    
 

